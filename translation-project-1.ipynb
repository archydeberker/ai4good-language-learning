{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "from spacy import displacy\n",
    "from IPython import display\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pg345.txt') as f:\n",
    "    dracula = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy download 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check we have the Spacy model downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spacy English model (sub in your own path using the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/Users/archy/anaconda/envs/translation/lib/python3.5/site-packages/en_core_web_sm/en_core_web_sm-2.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snipped = dracula.split('3 May. Bistritz')[-1][0:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Spacy POS to tag cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(snipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub in your Yandex API key below (go to https://translate.yandex.com), and run `export YANDEX_API_KEY=xxxxx` in your shell, where `xxxxx` is your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('YANDEX_API_KEY', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate',\n",
    "                 data={'key':api_key,\n",
    "                 'text':'hello',\n",
    "                 'lang':'en-fr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(word):\n",
    "    r = requests.post('https://translate.yandex.net/api/v1.5/tr.json/translate',\n",
    "                 data={'key':api_key,\n",
    "                 'text':word,\n",
    "                 'lang':'en-fr'})\n",
    "    \n",
    "    return ' '.join(r.json()['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try and just translate nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_output_to_html(html_doc, filename):\n",
    "    \n",
    "    full_doc = html_doc['header'] + html_doc['body'] + html_doc['footer']\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(full_doc)\n",
    "        \n",
    "    return filename\n",
    "\n",
    "def display_html(filename, width=200, height=200):\n",
    "    return IFrame(src=filename, width=width, height=height)\n",
    "\n",
    "def mark_translation(text, marker = 'em'):\n",
    "    \n",
    "    return '<' + marker + '>' + text + '</' + marker + '>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN' or token.pos_ == 'DET' or token.pos_ == 'ADJ':\n",
    "        output += get_translation(token.text_with_ws)\n",
    "    else:\n",
    "        output += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just translating nouns is ok, but jarring, because the determinants are missing e.g. 'one heure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't translate the determinants on their own either, going to need to do them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy we adopt here is to start accumulating a \"trace\" whenever we hit a determinant or an adjective. When we get to the noun, we translate. This seems reasonable because in English adjectives precede nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "trace = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'DET' or token.pos_ == 'ADJ':\n",
    "        trace.append(token.text_with_ws)\n",
    "        print(token.text_with_ws)\n",
    "    elif token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "        if len(trace) > 0:\n",
    "            trace.append(token.text_with_ws)\n",
    "            print('translating', trace)\n",
    "            output += mark_translation(get_translation(' '.join(trace)))\n",
    "            trace = []\n",
    "            print('___')\n",
    "        else:\n",
    "            output += mark_translation(get_translation(token.text_with_ws))\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        if token.pos_ == 'PUNCT' and len(trace) > 0:\n",
    "            output += mark_translation(get_translation(' '.join(trace)))\n",
    "            print('translating', trace)\n",
    "            trace = []\n",
    "\n",
    "\n",
    "        output += token.text_with_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering HTML externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tooltip_ref(text):\n",
    "    return f'<div class=\"htmltooltip\"> {text} </div>'\n",
    "\n",
    "def mark_translation_tooltip(text):\n",
    "    return f'<a href=\"#\" rel=\"htmltooltip\"> {text} </a>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for HTML rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = {'header': None, \n",
    "           'body': [],\n",
    "           'footer': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hod-raw.txt') as f:\n",
    "    hod = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hod_doc = nlp(hod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hod_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate a big chunk (this takes a whle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "footer = []\n",
    "trace = []\n",
    "for i, token in enumerate(hod_doc[:n_sample]):\n",
    "    if token.pos_ == 'DET' or token.pos_ == 'ADJ':\n",
    "        trace.append(token.text_with_ws)\n",
    "#         print(token.text_with_ws)\n",
    "    elif token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "        if len(trace) > 0:\n",
    "            trace.append(token.text_with_ws)\n",
    "#             print('translating', trace)\n",
    "            \n",
    "            text_ = ' '.join(trace)\n",
    "            output += [mark_translation_tooltip(get_translation(text_))]\n",
    "            footer.append(add_tooltip_ref(text_))\n",
    "            \n",
    "            trace = []\n",
    "#             print('___')\n",
    "        else:\n",
    "            output += [mark_translation_tooltip(get_translation(token.text_with_ws))]\n",
    "            footer.append(add_tooltip_ref(token.text_with_ws))\n",
    "    \n",
    "    elif len(trace) > 0:\n",
    "        # If we're still inside a nounphrase, keep accumulating\n",
    "        trace.append(token.text_with_ws)\n",
    "    else:\n",
    "        if token.pos_ == 'PUNCT' and len(trace) > 0:\n",
    "            # Terminate traces at punctuation, as these correspond to clause endings\n",
    "            text_ = ' '.join(trace)\n",
    "            output += [mark_translation_tooltip(get_translation(text_))]\n",
    "            footer.append(add_tooltip_ref(text_))\n",
    "#             print('translating', trace)\n",
    "            trace = []\n",
    "        else:\n",
    "            output += [token.text_with_ws]\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i} / {len(hod_doc)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_in_container(text):\n",
    "    return f\"<div class='container'> {text} </div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('header.html') as f: \n",
    "    html_doc['header'] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc['body'] = wrap_in_container(''.join(output))\n",
    "html_doc['footer'] = '\\n'.join(footer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_output_to_html(html_doc, 'test_doc.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html('test_doc.html', width = 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "translation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
